{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrgn9gjxckX8EgH56f+XZH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alok-abhishek/Quantizing-LLMs-and-inferencing-Quantized-model-from-HF/blob/master/Quantizing_LLMs_and_inferencing_Quantized_model_from_HF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This workbook covers following topics:**\n",
        "- What is Quantization\n",
        "- How to Qunatize a model\n",
        "- How to Inference from quantized model\n",
        "\n",
        "**Quantization techniques covered:**\n",
        "- llama.ccp\n",
        "- bnb\n",
        "- AWQ\n",
        "- ExLlamaV2\n",
        "- GPTQ\n",
        "\n",
        "ðŸ¤— For Qns or comments reach out to me [@alokabhishek](https://www.linkedin.com/in/alokabhishek/)."
      ],
      "metadata": {
        "id": "HBNz1xJYs5OJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Intro to Qunatization**"
      ],
      "metadata": {
        "id": "lGwBBwWTw9-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What is quantization of Large language model?***\n",
        "\n",
        "Quantization of Large Language Models (LLMs) is a technique used to reduce the computational and memory requirements of these models by converting their weights and activations from a high-precision 32-bit floating-point representation to a lower-precision format such as 8-bit or 4-bit integers. This process allows LLMs to be more efficiently run on hardware with limited computational resources, including mobile and IoT devices, without significantly compromising LLMâ€™s performance or accuracy.\n",
        "\n",
        "***What are the benefits of quantization in large language models***\n",
        "- Reduced Model Size / Memory Footprint\n",
        "- Faster Inference Speed / Increased Efficiency\n",
        "- Lower Power Consumption / Energy Efficiency - Suitable for mobile devices\n",
        "- Model Compression and Portability\n",
        "\n",
        "***What are different quantization techniques?***\n",
        "- Post-Training Quantization (PTQ)\n",
        "- Quantization-Aware Training (QAT)\n",
        "- Activation-Aware Weight Quantization (AWQ)\n",
        "- NF4 Quantization - BitsAndBytes\n",
        "- etc.\n",
        "\n",
        "***Different Options for Quantization:***\n",
        "- 16-bit (Float16)\n",
        "- 8-bit (Int8): for deploying models on edge devices or situations where computational resources are limited\n",
        "- 4-bit: Useful for extremely resource-constrained environments\n",
        "- 1-bit (Binary)\n",
        "- NF4 (4bit-NormalFloat): A specialized 4-bit format designed to efficiently represent a larger bit datatype. It includes steps like normalization, quantization, and dequantization to efficiently represent original 32-bit weights.Suitable for applications requiring a balance between model size reduction and maintaining higher accuracy than traditional 4-bit quantization.\n",
        "- etc."
      ],
      "metadata": {
        "id": "2pgPXi_ntfTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "vtbTyemDwFTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Quantize models using GGUF and llama.cpp**"
      ],
      "metadata": {
        "id": "SFC55vIYwWC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Useful links:\n",
        "- llama.cpp GitHub repo: [llama.cpp github repo](https://github.com/ggerganov/llama.cpp)\n",
        "- llama-cpp-python GitHub repo: https://github.com/abetlen/llama-cpp-python"
      ],
      "metadata": {
        "id": "4seqb87BjU9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **q2_k:** Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\n",
        "*   **q3_k_l:** Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\n",
        "*   **q3_k_m:** Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\n",
        "*   **q3_k_s:** Uses Q3_K for all tensors\n",
        "*   **q4_0:** Original quant method, 4-bit.\n",
        "*   **q4_1:** Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\n",
        "*   **q4_k_m:** Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\n",
        "*   **q4_k_s:** Uses Q4_K for all tensors\n",
        "*   **q5_0:** Higher accuracy, higher resource usage and slower inference.\n",
        "*   **q5_1:** Even higher accuracy, resource usage and slower inference.\n",
        "*   **q5_k_m:** Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\n",
        "*   **q5_k_s:** Uses Q5_K for all tensors\n",
        "*   **q6_k:** Uses Q8_K for all tensors\n",
        "*   **q8_0:** Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.\n",
        "\n"
      ],
      "metadata": {
        "id": "epG7RaodkMu_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bATydcXDjHDs"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n",
        "!pip install -r llama.cpp/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata, drive\n",
        "import torch\n",
        "import os\n",
        "from torch import bfloat16\n",
        "from huggingface_hub import login, HfApi, create_repo\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ],
      "metadata": {
        "id": "Q3HAjnKmjQgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN = userdata.get('HUGGING_FACE_API_KEY')\n",
        "login(token=HF_TOKEN)\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "username = api.whoami()['name']\n",
        "print(username)"
      ],
      "metadata": {
        "id": "RC8DQqTBjQje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantize meta-llama/Llama-2-7b-chat-hf**"
      ],
      "metadata": {
        "id": "wtXwYlcpkV0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model ID for the desired model\n",
        "model_id_llama = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "quantization_methods = [\"q5_k_m\", \"q4_k_m\"]"
      ],
      "metadata": {
        "id": "YXjUphkzjQma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name =  model_id_llama.split(\"/\")[-1]\n",
        "print(model_name)\n",
        "quant_name =  model_id_llama.split(\"/\")[-1] + \"-GGUF\"\n",
        "print(quant_name)\n",
        "quant_repo_id = f\"{username}/{quant_name}\"\n",
        "print(quant_repo_id)"
      ],
      "metadata": {
        "id": "ZSSgSBVIjQpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git-lfs install"
      ],
      "metadata": {
        "id": "Brtkru7sjQsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download model\n",
        "!git clone https://{username}:{HF_TOKEN}@huggingface.co/{model_id_llama}\n"
      ],
      "metadata": {
        "id": "wf6M1tRmjQvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to fp16\n",
        "fp16 = f\"{model_name}/{model_name.lower()}.fp16.bin\"\n",
        "!python llama.cpp/convert.py {model_name} --outtype f16 --outfile {fp16}\n",
        "\n",
        "# Quantize the model for each method in the QUANTIZATION_METHODS list\n",
        "for method in quantization_methods:\n",
        "    qtype = f\"{model_name}/{model_name.lower()}.{method.upper()}.gguf\"\n",
        "    !./llama.cpp/quantize {fp16} {qtype} {method}"
      ],
      "metadata": {
        "id": "N2V3kCrwjQyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty repo\n",
        "api.create_repo(\n",
        "    repo_id = quant_repo_id,\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        "    token=HF_TOKEN,\n",
        "    private=True\n",
        ")"
      ],
      "metadata": {
        "id": "ZYMn36I1jQ0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload gguf files\n",
        "api.upload_folder(\n",
        "    folder_path=model_name,\n",
        "    repo_id=quant_repo_id,\n",
        "    token=HF_TOKEN\n",
        ")"
      ],
      "metadata": {
        "id": "kcfazFP9jQ4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inferencing GGUF type models**"
      ],
      "metadata": {
        "id": "NJj6Ck9zk5JV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**using llama_cpp (recommended)**"
      ],
      "metadata": {
        "id": "FfgJ1SvolNg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "import os\n",
        "import dotenv\n",
        "from huggingface_hub import login, HfApi"
      ],
      "metadata": {
        "id": "9ZpBtyfhlFTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dotenv.load_dotenv()\n",
        "\n",
        "HF_TOKEN = os.environ.get(\"HUGGING_FACE_API_KEY\")\n",
        "login(token=HF_TOKEN)\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "username = api.whoami()[\"name\"]"
      ],
      "metadata": {
        "id": "5gEw_RkGjQ7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"alokabhishek/Llama-2-7b-chat-hf-GGUF\"\n",
        "filename = \"llama-2-7b-chat-hf.Q4_K_M.gguf\""
      ],
      "metadata": {
        "id": "d6uY0ZHLjQ-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Tell me a funny joke about Large Language Models meeting a Blackhole in an intergalactic Bar.\""
      ],
      "metadata": {
        "id": "F2Qzr5O1jRBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Llama.from_pretrained(\n",
        "    repo_id=repo_id,\n",
        "    filename=filename,\n",
        "    verbose=False,\n",
        ")"
      ],
      "metadata": {
        "id": "eAAWi8r-jREn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_response = llm.create_chat_completion(\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    temperature=0.85,\n",
        "    top_p=0.8,\n",
        "    top_k=50,\n",
        "    repeat_penalty=1.01,\n",
        ")"
      ],
      "metadata": {
        "id": "PkLNFvr1jRHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_respose_formatted = llm_response[\"choices\"][0][\"message\"][\"content\"]"
      ],
      "metadata": {
        "id": "j-S8xzYLjRKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_respose_formatted)"
      ],
      "metadata": {
        "id": "NHdfRetQjRO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using ctransformers (ctransformers liabrary has not been updated in last 6+ month so I dont recommend using it right now as it does support some of the newwer models and frameworks)**"
      ],
      "metadata": {
        "id": "VrwZEAtll0MU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ctransformers[cuda]>=0.2.24\n",
        "! pip install -U sentence-transformers\n",
        "! pip install transformers huggingface_hub torch"
      ],
      "metadata": {
        "id": "D0xyWWpYjRU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ctransformers import AutoModelForCausalLM\n",
        "from transformers import pipeline, AutoModel\n",
        "from huggingface_hub import login, HfApi, create_repo\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from google.colab import userdata, drive\n",
        "import os"
      ],
      "metadata": {
        "id": "Qt2N9fLujRX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN = userdata.get('HUGGING_FACE_API_KEY')\n",
        "login(token=HF_TOKEN)\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "username = api.whoami()['name']\n",
        "print(username)"
      ],
      "metadata": {
        "id": "6GMT6tGEjRa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_mistral = AutoModelForCausalLM.from_pretrained(\n",
        "    \"alokabhishek/Mistral-7B-Instruct-v0.2-GGUF\",\n",
        "    model_file=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
        "    model_type=\"mistral\", gpu_layers=50, hf=True\n",
        ")"
      ],
      "metadata": {
        "id": "cKjyWjADjReB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_mistral = AutoTokenizer.from_pretrained(\n",
        "    \"alokabhishek/Mistral-7B-Instruct-v0.2-GGUF\", use_fast=True\n",
        ")"
      ],
      "metadata": {
        "id": "Usmt2f3tmMYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline\n",
        "pipe_mistral = pipeline(model=model_mistral, tokenizer=tokenizer_mistral, task='text-generation')"
      ],
      "metadata": {
        "id": "-sXrZ-XumMbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_mistral = \"Tell me a funny joke about Large Language Models meeting a Blackhole in an intergalactic Bar.\""
      ],
      "metadata": {
        "id": "e78lqDMTmMeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_mistral = pipe_mistral(prompt_mistral, max_new_tokens=512)"
      ],
      "metadata": {
        "id": "AZT5xv6_mMgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_mistral[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "-QNeq7rEmMjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Quantize models using bitsandbytes (bnb)**"
      ],
      "metadata": {
        "id": "KTPBLtPewfqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- Hugging Face Blog post on 4-bit quantization using bitsandbytes: [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)\n",
        "\n",
        "- bitsandbytes github repo: [bitsandbytes github repo](https://github.com/TimDettmers/bitsandbytes)\n"
      ],
      "metadata": {
        "id": "hWg2E0vnm3YM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes accelerate torch huggingface_hub\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git"
      ],
      "metadata": {
        "id": "YM0bA1e7mMmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata, drive\n",
        "import torch\n",
        "import os\n",
        "from torch import bfloat16\n",
        "from huggingface_hub import login, HfApi, create_repo\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "2aDN85IfmMpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN = userdata.get('HUGGING_FACE_API_KEY')\n",
        "login(token=HF_TOKEN)\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "username = api.whoami()['name']\n",
        "print(username)"
      ],
      "metadata": {
        "id": "KQblb8bZmMsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model ID for the desired model\n",
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\""
      ],
      "metadata": {
        "id": "L7-0H4xxmMvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the quantization configuration for the model\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "-g6WeEw7mMx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer associated with the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "6Msx-sesmM0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model using the model ID and quantization configuration\n",
        "model_4bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)"
      ],
      "metadata": {
        "id": "rDGLpjxrmM3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_name =  model_id.split(\"/\")[-1] + \"-bnb-4bit\"\n",
        "print(quant_name)\n",
        "quant_repo_id = f\"{username}/{quant_name}\"\n",
        "print(quant_repo_id)"
      ],
      "metadata": {
        "id": "EVocpfGfmM6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty repo\n",
        "api.create_repo(\n",
        "    repo_id = quant_repo_id,\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        "    token=HF_TOKEN,\n",
        "    private=True\n",
        ")"
      ],
      "metadata": {
        "id": "z3foaLkimM9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(quant_name)"
      ],
      "metadata": {
        "id": "VhHV0m-_mNAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_4bit.push_to_hub(quant_name, token=True, use_safetensors=True)"
      ],
      "metadata": {
        "id": "7fY9MZwAmNDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api.upload_folder(folder_path=quant_name,repo_id=quant_repo_id,token=HF_TOKEN)"
      ],
      "metadata": {
        "id": "cgS7HuXfmNF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inferencing bnb quantized model from HF hub**"
      ],
      "metadata": {
        "id": "y1qLdksLn27s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes accelerate torch huggingface_hub\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "2q8EgSoHmNIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata, drive\n",
        "import torch\n",
        "import os\n",
        "from torch import bfloat16\n",
        "from huggingface_hub import login, HfApi, create_repo\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig, LlamaForCausalLM"
      ],
      "metadata": {
        "id": "AzIr7vrgmNLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN = userdata.get('HUGGING_FACE_API_KEY')\n",
        "login(token=HF_TOKEN)\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "username = api.whoami()['name']\n",
        "print(username)"
      ],
      "metadata": {
        "id": "I4vXxYDmmNOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id_falcon = \"alokabhishek/falcon-7b-instruct-bnb-4bit\""
      ],
      "metadata": {
        "id": "MCzEIluWmNRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_falcon = AutoTokenizer.from_pretrained(model_id_falcon, use_fast=True)"
      ],
      "metadata": {
        "id": "AxzakqaCoD3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_falcon = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id_falcon,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "LXzhZj6UoD6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline\n",
        "pipe_falcon = pipeline(model=model_falcon, tokenizer=tokenizer_falcon, task='text-generation')"
      ],
      "metadata": {
        "id": "ob38XdFgoD9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_falcon = \"Tell me a funny joke about Large Language Models meeting a Blackhole in an intergalactic Bar.\""
      ],
      "metadata": {
        "id": "rrYsHsfhoEAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_falcon = pipe_falcon(prompt_falcon, max_new_tokens=512)"
      ],
      "metadata": {
        "id": "Amz19ElboEC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_falcon[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "HxEFdmF1mNUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Quantize models using ExLlamaV2**"
      ],
      "metadata": {
        "id": "ikEezStDwnSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- ExLlamaV2 github repo: [ExLlamaV2 github repo](https://github.com/turboderp/exllamav2)"
      ],
      "metadata": {
        "id": "vKoLjX2CollY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/turboderp/exllamav2"
      ],
      "metadata": {
        "id": "tLq5PYp_mNXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cd exllamav2"
      ],
      "metadata": {
        "id": "TaxD3FXko-8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "2Lb6kRZGo_EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ."
      ],
      "metadata": {
        "id": "NrlQZmU4o_LM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata, drive\n",
        "from torch import bfloat16\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from huggingface_hub import login, HfApi, create_repo\n",
        "import locale\n",
        "import torch\n",
        "import os"
      ],
      "metadata": {
        "id": "Sei8RTHPoein"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN = userdata.get('HUGGING_FACE_API_KEY')\n",
        "login(token=HF_TOKEN)\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "username = api.whoami()['name']\n",
        "print(username)"
      ],
      "metadata": {
        "id": "CxjkVVZioeli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model ID for the desired model\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "BPW = 5.0"
      ],
      "metadata": {
        "id": "iVXYSAhIoeoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name =  model_id.split(\"/\")[-1]\n",
        "print(model_name)\n",
        "quant_name =  model_id.split(\"/\")[-1] + f\"-{BPW:.1f}-bpw-exl2\"\n",
        "print(quant_name)\n",
        "quant_repo_id = f\"{username}/{quant_name}\"\n",
        "print(quant_repo_id)"
      ],
      "metadata": {
        "id": "Jm72yuAKoerY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git-lfs install"
      ],
      "metadata": {
        "id": "-oOpw_-7oeua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://{username}:{HF_TOKEN}@huggingface.co/{model_id}"
      ],
      "metadata": {
        "id": "APdkZmDJoexQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv {model_name} base_model"
      ],
      "metadata": {
        "id": "QgFWb3QYoe0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/base_model/*.bin"
      ],
      "metadata": {
        "id": "czQ1e-rhoe3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "!wget https://huggingface.co/datasets/wikitext/resolve/9a9e482b5987f9d25b3a9b2883fc6cc9fd8071b3/wikitext-103-v1/wikitext-test.parquet"
      ],
      "metadata": {
        "id": "LpoPQytToe6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir quant"
      ],
      "metadata": {
        "id": "D2TMAcmjoe83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp /content/base_model/config.json /content/quant/config.json"
      ],
      "metadata": {
        "id": "gaTfTjSgoe_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantize model\n",
        "\n",
        "!python exllamav2/convert.py \\\n",
        "    -i base_model \\\n",
        "    -o quant \\\n",
        "    -c wikitext-test.parquet \\\n",
        "    -b {BPW}"
      ],
      "metadata": {
        "id": "ATLl--txofC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove out_tensor dir\n",
        "!rm -rf /content/quant/out_tensor"
      ],
      "metadata": {
        "id": "8OV4ALCqofF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy files\n",
        "!rsync -av --exclude='*.safetensors' --exclude='.*' ./base_model/ ./quant/"
      ],
      "metadata": {
        "id": "3sVGHe3CofIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty repo\n",
        "api.create_repo(\n",
        "    repo_id = quant_repo_id,\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        "    token=HF_TOKEN,\n",
        "    private=True\n",
        ")"
      ],
      "metadata": {
        "id": "yLvCF897ofLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload files\n",
        "api.upload_folder(\n",
        "    folder_path=\"quant\",\n",
        "    repo_id=quant_repo_id,\n",
        "    token=HF_TOKEN\n",
        ")"
      ],
      "metadata": {
        "id": "Wa28boyRofOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inferencing ExLlamaV2 Quantized Models**"
      ],
      "metadata": {
        "id": "iDhpGLOPpjFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model ID for the desired model\n",
        "model_id = \"alokabhishek/Llama-2-7b-chat-hf-5.0-bpw-exl2\""
      ],
      "metadata": {
        "id": "0VqNW6vKw2E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name =  model_id.split(\"/\")[-1]\n",
        "print(model_name)"
      ],
      "metadata": {
        "id": "4SlUP1QRwxYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://{username}:{HF_TOKEN}@huggingface.co/{model_id} {model_name}"
      ],
      "metadata": {
        "id": "syh-PXDwqrmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
        "from exllamav2 import (\n",
        "    ExLlamaV2,\n",
        "    ExLlamaV2Config,\n",
        "    ExLlamaV2Cache,\n",
        "    ExLlamaV2Tokenizer,\n",
        ")\n",
        "from exllamav2.generator import ExLlamaV2BaseGenerator, ExLlamaV2Sampler"
      ],
      "metadata": {
        "id": "QtSFXaf0ofRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_directory = \"../../quant/Llama-2-7b-chat-hf-5.0-bpw-exl2/\""
      ],
      "metadata": {
        "id": "UefgbcxtofUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = ExLlamaV2Config(model_directory)\n",
        "model = ExLlamaV2(config)\n",
        "cache = ExLlamaV2Cache(model, lazy=True)\n",
        "model.load_autosplit(cache)\n",
        "tokenizer = ExLlamaV2Tokenizer(config)"
      ],
      "metadata": {
        "id": "yaWD8-YKofW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize generator\n",
        "generator = ExLlamaV2BaseGenerator(model, cache, tokenizer)"
      ],
      "metadata": {
        "id": "1tCoiUofofZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "settings = ExLlamaV2Sampler.Settings()\n",
        "settings.temperature = 0.85\n",
        "settings.top_k = 50\n",
        "settings.top_p = 0.8\n",
        "settings.token_repetition_penalty = 1.01\n",
        "settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])\n",
        "max_new_tokens = 512"
      ],
      "metadata": {
        "id": "0SMdMUADofcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Tell me a funny joke about Large Language Models meeting a Blackhole in an intergalactic Bar.\""
      ],
      "metadata": {
        "id": "gUZaZBz1mNab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator.warmup()"
      ],
      "metadata": {
        "id": "8FkoVyZfmNdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = generator.generate_simple(prompt, settings, max_new_tokens, seed=1234)"
      ],
      "metadata": {
        "id": "3K0mt8TZjRhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "id": "smINE9c1jRkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Quantize models using AutoAWQ**"
      ],
      "metadata": {
        "id": "fIy38JW_wuBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- AutoAWQ github repo: [AutoAWQ github repo](https://github.com/casper-hansen/AutoAWQ/tree/main)\n",
        "- MIT-han-lab llm-aws github repo:  [MIT-han-lab llm-aws github repo](https://github.com/mit-han-lab/llm-awq/tree/main)"
      ],
      "metadata": {
        "id": "ElLjG6pwrAhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autoawq"
      ],
      "metadata": {
        "id": "TBeolqudjRns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from google.colab import userdata, drive\n",
        "from torch import bfloat16\n",
        "from huggingface_hub import login, HfApi, create_repo\n",
        "from transformers import AutoTokenizer, AwqConfig, AutoConfig\n",
        "from awq import AutoAWQForCausalLM\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "pOzWiaRpq6wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN = userdata.get('HUGGING_FACE_API_KEY')\n",
        "login(token=HF_TOKEN)\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "username = api.whoami()['name']\n",
        "print(username)"
      ],
      "metadata": {
        "id": "64uQ8k2Gq6zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model ID for the desired model\n",
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\""
      ],
      "metadata": {
        "id": "NkekoV3zq645"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name =  model_id.split(\"/\")[-1]\n",
        "print(model_name)\n",
        "quant_name =  model_id.split(\"/\")[-1] + \"-4bit-AWQ\"\n",
        "print(quant_name)\n",
        "quant_repo_id = f\"{username}/{quant_name}\"\n",
        "print(quant_repo_id)"
      ],
      "metadata": {
        "id": "tzOaon4Pq67x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }"
      ],
      "metadata": {
        "id": "t3rNSlbWq6-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "model = AutoAWQForCausalLM.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "-UZiL8zqq7Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "nvzBmOVmq7EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantize\n",
        "model.quantize(tokenizer, quant_config=quant_config)"
      ],
      "metadata": {
        "id": "MQUmiu5Uq7HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save quantized model\n",
        "model.save_quantized(quant_name)"
      ],
      "metadata": {
        "id": "i1tHjFedq7KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save quantized model\n",
        "tokenizer.save_pretrained(quant_name)"
      ],
      "metadata": {
        "id": "cwPJuBcdq7NL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty repo\n",
        "api.create_repo(\n",
        "    repo_id = quant_repo_id,\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        "    token=HF_TOKEN,\n",
        "    private=True\n",
        ")"
      ],
      "metadata": {
        "id": "NIMDB8Hbq7QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload files\n",
        "api.upload_folder(\n",
        "    folder_path=quant_name,\n",
        "    repo_id=quant_repo_id,\n",
        "    token=HF_TOKEN\n",
        ")"
      ],
      "metadata": {
        "id": "Qxsxzt5DrSU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference AWQ Quantization of Models**"
      ],
      "metadata": {
        "id": "wE9J79I3rXjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autoawq\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "hOX2WzwsrSay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata, drive\n",
        "import torch\n",
        "import os\n",
        "from torch import bfloat16\n",
        "from huggingface_hub import login, HfApi, create_repo\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from awq import AutoAWQForCausalLM"
      ],
      "metadata": {
        "id": "HpElnplcrSdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN = userdata.get('HUGGING_FACE_API_KEY')\n",
        "login(token=HF_TOKEN)\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "username = api.whoami()['name']\n",
        "print(username)"
      ],
      "metadata": {
        "id": "ZjO9ZwqDrSgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id_llama = \"alokabhishek/Mistral-7B-Instruct-v0.2-4bit-AWQ\""
      ],
      "metadata": {
        "id": "10YSqkLUrSji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_llama = AutoTokenizer.from_pretrained(model_id_llama, use_fast=True)"
      ],
      "metadata": {
        "id": "1G-WYA5TrSme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_llama = AutoAWQForCausalLM.from_quantized(model_id_llama, fuse_layer=True, trust_remote_code = False, safetensors = True)"
      ],
      "metadata": {
        "id": "keoQMIr8rSpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_llama = \"Tell me a funny joke about Large Language Models meeting a Blackhole in an intergalactic Bar.\""
      ],
      "metadata": {
        "id": "uiyZN0aorSsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fromatted_prompt = f'''<s> [INST] You are a helpful, and fun loving assistant. Always answer as jestfully as possible.[/INST] </s> [INST] {prompt_llama}[/INST]'''"
      ],
      "metadata": {
        "id": "GuZXwOUWrSvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer_llama(fromatted_prompt, return_tensors=\"pt\").input_ids.cuda()"
      ],
      "metadata": {
        "id": "mFfTNAYzrSyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_output = model_llama.generate(tokens, do_sample=True, temperature=1.7, top_p=0.95, top_k=40, max_new_tokens=512)"
      ],
      "metadata": {
        "id": "k6KtFmHYrS08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer_llama.decode(generation_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "j7M0fgearS34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Quantize models using GPTQ**"
      ],
      "metadata": {
        "id": "BSpKQSmpw17p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPTQ Quantization 4-bit LLM**"
      ],
      "metadata": {
        "id": "vzJURsOZsZR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!BUILD_CUDA_EXT=0 pip install -q auto-gptq transformers"
      ],
      "metadata": {
        "id": "svj1OeJWsY2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import locale\n",
        "import torch\n",
        "import os\n",
        "from google.colab import userdata, drive\n",
        "from torch import bfloat16\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from huggingface_hub import login, HfApi, create_repo\n",
        "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "from datasets import load_dataset, concatenate_datasets\n"
      ],
      "metadata": {
        "id": "c1xAGVB3rS6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN = userdata.get('HUGGING_FACE_API_KEY')\n",
        "login(token=HF_TOKEN)\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "username = api.whoami()['name']\n",
        "print(username)"
      ],
      "metadata": {
        "id": "zRWjSqC-rS9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model ID for the desired model\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\""
      ],
      "metadata": {
        "id": "Qc4WxaCiq7Vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name =  model_id.split(\"/\")[-1]\n",
        "print(model_name)\n",
        "quant_name =  model_id.split(\"/\")[-1] + \"-GPTQ\"\n",
        "print(quant_name)\n",
        "quant_repo_id = f\"{username}/{quant_name}\"\n",
        "print(quant_repo_id)"
      ],
      "metadata": {
        "id": "Dtj8Zt-Kq7Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load quantize config, model and tokenizer\n",
        "quantize_config = BaseQuantizeConfig(\n",
        "    bits=4,\n",
        "    group_size=128,\n",
        "    damp_percent=0.01,\n",
        "    desc_act=False,\n",
        ")\n"
      ],
      "metadata": {
        "id": "tp_jOTYDq7bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "jNfbRvlNsjx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base parameters\n",
        "base_samples = 1024\n",
        "file_numbers = [1, 6, 10]  # Specify the file numbers to include\n",
        "\n",
        "# Generate the list of data files based on the specified file numbers\n",
        "data_files = [f\"en/c4-train.0000{num}-of-01024.json.gz\" if num < 10 else f\"en/c4-train.000{num}-of-01024.json.gz\" for num in file_numbers]\n",
        "\n",
        "# Calculate the total number of samples\n",
        "n_samples = base_samples * len(file_numbers)\n",
        "\n",
        "# Load and concatenate the datasets\n",
        "datasets = []\n",
        "for file in data_files:\n",
        "    dataset = load_dataset(\"allenai/c4\", data_files=file, split=f\"train[:{base_samples*5}]\")\n",
        "    datasets.append(dataset)\n",
        "# concatenate dataset\n",
        "data = concatenate_datasets(datasets)"
      ],
      "metadata": {
        "id": "lMLj3M_Ysj1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the concatenated data\n",
        "tokenized_data = tokenizer(\"\\n\\n\".join(data['text']), return_tensors='pt')"
      ],
      "metadata": {
        "id": "bq3exDFysj4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a maximum sequence length\n",
        "max_length = 1024  # Adjust based on your requirements\n",
        "\n",
        "examples_ids = []\n",
        "for _ in range(n_samples):\n",
        "    # Ensure the random start index selection is within bounds\n",
        "    i = random.randint(0, max(tokenized_data.input_ids.shape[1] - max_length - 1, 0))\n",
        "    j = i + max_length\n",
        "    input_ids = tokenized_data.input_ids[:, i:j]\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "    examples_ids.append({'input_ids': input_ids, 'attention_mask': attention_mask})"
      ],
      "metadata": {
        "id": "qdT4368Fsj6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantize with GPTQ\n",
        "model.quantize(\n",
        "    examples_ids,\n",
        "    batch_size=1,\n",
        "    use_triton=True,\n",
        ")"
      ],
      "metadata": {
        "id": "pwsfq3wtq7hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model and tokenizer\n",
        "model.save_quantized(quant_name, use_safetensors=True)"
      ],
      "metadata": {
        "id": "WVWz3VYqjRqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(quant_name)"
      ],
      "metadata": {
        "id": "J2YMtAOFste3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty repo\n",
        "api.create_repo(\n",
        "    repo_id = quant_repo_id,\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        "    token=HF_TOKEN,\n",
        "    private=True\n",
        ")"
      ],
      "metadata": {
        "id": "B2VZTDMWsthw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload files\n",
        "api.upload_folder(\n",
        "    folder_path=quant_name,\n",
        "    repo_id=quant_repo_id,\n",
        "    token=HF_TOKEN\n",
        ")"
      ],
      "metadata": {
        "id": "hT2NvlMUstkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pZbiBqVRstnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KeRQ1hq7s1zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3t8_EVm0s12R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZcgL4dyUs147"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GM-mqiaFs1-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U4xh1_Qzs2Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q3qaU_wlstqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fHKgLJ6pjRtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bbagZhTajRwJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}